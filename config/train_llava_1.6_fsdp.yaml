name: test-run
target: modules.train_llava.setup
unwrap_fabric_init: true

model_params:
  trust_remote_code: true
  attn_implementation: sdpa

# finetune config
model_config:
  version: vicuna_v1
  image_aspect_ratio: "anyres"
  image_crop_resolution: 224
  image_split_resolution: 224
  mm_hidden_size: 1024
  mm_patch_merge_type: "spatial_unpad"
  mm_projector_lr: null
  mm_projector_type: "mlp2x_gelu"
  mm_resampler_type: null
  mm_use_im_patch_token: false
  mm_use_im_start_end: false
  mm_vision_select_feature: "patch"
  mm_vision_select_layer: -2
  mm_vision_tower: "openai/clip-vit-large-patch14-336"
  mm_vision_tower_lr: 2e-06
  tune_mm_mlp_adapter: true
  tune_mm_vision_resampler: false
  unfreeze_mm_vision_tower: true # actually its load_vision_tower
  use_mm_proj: true
  hidden_size: 5120
  image_grid_pinpoints: [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]

use_lora: false
q_lora: false
lora_params:
  r: 128
  lora_alpha: 256
  target_modules: ... # will inherit from the model_params
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

trainer:
  model_path: /workspace/vicuna-13b-1.5-local
  batch_size: 32
  seed: 1138
  wandb_id: "qwen"
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0

  save_format: diffusers
  checkpoint_dir: checkpoint
  checkpoint_freq: 1
  checkpoint_steps: 5000
  save_weights_only: True
  eval_samples: 1000
  eval_steps: 1000
  eval_epoch: 1
  max_epochs: 60
  max_steps: -1
  
lightning:
  accelerator: gpu
  devices: -1
  strategy: common.fsdp._strategy
  precision: bf16-mixed

dataset:
  name: data.llava_dataset.LazySupervisedDataset
  model_max_length: 2048
  data_path: /workspace/llava-pretrain/blip_laion_cc_sbu_558k.json
  image_folder: /workspace/llava-pretrain/images
  
optimizer:
  name: torch.optim.AdamW
  params:
    lr: 4e-5
    weight_decay: 1e-2

scheduler:
  name: transformers.get_constant_schedule_with_warmup
  params:
    num_warmup_steps: 50
    last_epoch: -1

sampling:
  enabled: false
  max_length: 200
  every_n_steps: 50
  every_n_epochs: 1
  prompts:
    - "what is the result of 1+1"
    - "what is the result of 2*591"
    - "what is the result of 3^2"
    - "what is the result of sqrt(128)"
